{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "8zloECyTntYL",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Semi-supervised image classification using Pseudo Labeling CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5LFxiHFntYP"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsEAhpOsntYP"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "\n",
    "# Make sure we are able to handle large datasets\n",
    "import resource\n",
    "\n",
    "low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import keras\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress\n",
    "from keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Set seed for evaluation purpose (remove in production)\n",
    "keras.utils.set_random_seed(5)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(5)\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "np.random.seed(5)\n",
    "tf.random.set_seed(5)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfHfEU0WntYQ"
   },
   "source": [
    "## Hyperparameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYiAk4QOntYQ"
   },
   "outputs": [],
   "source": [
    "# Dataset hyperparameters\n",
    "unlabeled_dataset_path = \"../data_ssl/unlabeled/\"\n",
    "labeled_dataset_path = \"../data_ssl/train/\"\n",
    "\n",
    "unlabeled_dataset_size = sum(\n",
    "    [len(files) for r, d, files in os.walk(unlabeled_dataset_path)]\n",
    ")\n",
    "labeled_dataset_size = sum(\n",
    "    [len(files) for r, d, files in os.walk(labeled_dataset_path)]\n",
    ")\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "width = 224\n",
    "num_epochs = 50\n",
    "batch_size = 30\n",
    "\n",
    "print(\"Unlabeled Images: \" + str(unlabeled_dataset_size))\n",
    "print(\"Labeled Images: \" + str(labeled_dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOoK85X1ntYQ"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4h27xPSntYR"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    labeled_train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        labeled_dataset_path,\n",
    "        validation_split=0.4,\n",
    "        subset=\"both\",\n",
    "        seed=5,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        color_mode=\"rgb\",\n",
    "        label_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    # Define test dataset\n",
    "    test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"../data_ssl/test_labeled/\",\n",
    "        seed=5,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        color_mode=\"rgb\",\n",
    "        label_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    return labeled_train_ds, val_ds, test_dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "labeled_train_dataset, validation_dataset, test_dataset = prepare_dataset()\n",
    "\n",
    "num_classes = len(labeled_train_dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF68BiqsntYR"
   },
   "source": [
    "## Image augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pl7k3VrpntYR"
   },
   "outputs": [],
   "source": [
    "# Plot example classes\n",
    "def visualize_augmentations(num_images):\n",
    "    # Sample a batch from a dataset\n",
    "    images = next(iter(labeled_train_dataset))[0][:num_images]\n",
    "\n",
    "    augmented_images = zip(\n",
    "        keras.Sequential([layers.Rescaling(1 / 255)])(images),\n",
    "    )\n",
    "    row_titles = [\n",
    "        \"Images:\",\n",
    "    ]\n",
    "    plt.figure(figsize=(num_images * 1.6, 2), dpi=100)\n",
    "    for column, image_row in enumerate(augmented_images):\n",
    "        for row, image in enumerate(image_row):\n",
    "            plt.subplot(1, num_images, row * num_images + column + 1)\n",
    "            plt.imshow(image)\n",
    "            if column == 0:\n",
    "                plt.title(row_titles[row], loc=\"left\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_augmentations(num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWwjJX0pntYS"
   },
   "source": [
    "## Supervised baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tK9FwctOntYT"
   },
   "outputs": [],
   "source": [
    "# Baseline supervised training with random initialization\n",
    "pretrained_model = tf.keras.applications.efficientnet_v2.EfficientNetV2S(\n",
    "    input_shape=(img_width, img_height, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling=\"avg\",\n",
    ")\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "inputs = pretrained_model.input\n",
    "x = tf.keras.layers.Dense(132, activation=\"relu\")(pretrained_model.output)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "baseline_model = tf.keras.Model(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    name=\"baseline_model\",\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "baseline_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n",
    "]\n",
    "\n",
    "# Fit model (training)\n",
    "baseline_history = baseline_model.fit(\n",
    "    labeled_train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(baseline_history.history[\"val_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DXAQJkAntYT"
   },
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SXehP8NntYT"
   },
   "outputs": [],
   "source": [
    "# Load images from files\n",
    "image_filepath = []\n",
    "\n",
    "for file in pathlib.Path(unlabeled_dataset_path).rglob(\"*.jpg\"):\n",
    "    if file.is_file():\n",
    "        image_filepath.append(str(file))\n",
    "\n",
    "df = pd.DataFrame({\"image_filepath\": image_filepath})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SXehP8NntYT"
   },
   "outputs": [],
   "source": [
    "max_count = len(df)\n",
    "\n",
    "f = IntProgress(min=0, max=max_count)  # instantiate the bar\n",
    "display(f)  # display the bar\n",
    "\n",
    "unlabeled_train_dataset = []\n",
    "pseudo_labels = []\n",
    "\n",
    "# Predict pseudo labels\n",
    "for index, row in df.iterrows():\n",
    "    image = cv2.imread(row[\"image_filepath\"])\n",
    "    if image.shape != (img_width, img_height, 3):\n",
    "        image = cv2.resize(image, (img_width, img_height))\n",
    "    image = image.reshape((1, img_width, img_height, 3))\n",
    "    pseudo_labels.append(baseline_model.predict(image, verbose=0))\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions and model for later use\n",
    "baseline_model.save(\"pseudo2_eff.keras\")\n",
    "np.save(\"pseudo2_eff.npy\", pseudo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SXehP8NntYT"
   },
   "outputs": [],
   "source": [
    "# Define finetuned model\n",
    "folder = labeled_dataset_path\n",
    "labels = sorted(\n",
    "    [name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))]\n",
    ")\n",
    "\n",
    "pretrained_finetuned_model = tf.keras.applications.efficientnet_v2.EfficientNetV2S(\n",
    "    input_shape=(img_width, img_height, 3), include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    ")\n",
    "pretrained_finetuned_model.trainable = False\n",
    "\n",
    "inputs = pretrained_finetuned_model.input\n",
    "x = tf.keras.layers.Dense(640, activation=\"relu\")(pretrained_finetuned_model.output)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(320, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "finetuned_model = tf.keras.Model(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    name=\"finetuned_model\",\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "finetuned_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "pseudo_labels = np.load(\"pseudo2_eff.npy\")\n",
    "baseline_model = keras.models.load_model(\"pseudo2_eff.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SXehP8NntYT"
   },
   "outputs": [],
   "source": [
    "# Select most confident pseudo-label predictions\n",
    "confident = []\n",
    "most_confident = []\n",
    "not_confident_indices = []\n",
    "labels_list = []\n",
    "for i, conf in enumerate(pseudo_labels):\n",
    "    conf = conf[0]\n",
    "    index = np.argmax(conf)\n",
    "    confident.append(index)\n",
    "    labels_list.append(labels[index])\n",
    "    if conf[index] > 0.995:\n",
    "        most_confident.append(labels[index])\n",
    "    else:\n",
    "        not_confident_indices.append(i)\n",
    "\n",
    "print(len(confident))\n",
    "print(len(most_confident))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unconfident predictions\n",
    "data_df = df.drop(not_confident_indices).reset_index(drop=True)\n",
    "data_df[\"label\"] = most_confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize class distribution\n",
    "value_counts = data_df[\"label\"].value_counts().sort_index()\n",
    "value_counts_normalized = data_df[\"label\"].value_counts(normalize=True).sort_index()\n",
    "print(value_counts)\n",
    "print(data_df)\n",
    "min = value_counts.min()\n",
    "for i, count in enumerate(value_counts):\n",
    "    drop_indices = np.random.choice(\n",
    "        data_df.loc[data_df[\"label\"] == labels[i]].index,\n",
    "        max(0, count - int(min * math.pow(value_counts_normalized.iloc[i] + 1, 2))),\n",
    "        replace=False,\n",
    "    )\n",
    "    data_df = data_df.drop(drop_indices).reset_index(drop=True)\n",
    "print(data_df[\"label\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from files\n",
    "image_filepath = []\n",
    "anomaly_class = []\n",
    "\n",
    "# Concenate labeled and pseudo-labeled images\n",
    "for file in pathlib.Path(labeled_dataset_path).rglob(\"*.jpg\"):\n",
    "    if file.is_file():\n",
    "        image_filepath.append(str(file))\n",
    "        anomaly_class.append(str(file).split(\"/\")[3])\n",
    "for i, x in enumerate(image_filepath):\n",
    "    data_df.loc[df.index.max() + i] = [image_filepath[i], anomaly_class[i]]\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled and unlabeled images in TensorFlow Dataset and apply augmentation\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "data = generator.flow_from_dataframe(\n",
    "    dataframe=data_df,\n",
    "    x_col=\"image_filepath\",\n",
    "    y_col=\"label\",\n",
    "    subset=\"training\",\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    "    classes=labels,\n",
    "    batch_size=batch_size,\n",
    "    seed=5,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "data_val = generator.flow_from_dataframe(\n",
    "    dataframe=data_df,\n",
    "    x_col=\"image_filepath\",\n",
    "    y_col=\"label\",\n",
    "    subset=\"validation\",\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    "    classes=labels,\n",
    "    batch_size=batch_size,\n",
    "    seed=5,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SXehP8NntYT"
   },
   "outputs": [],
   "source": [
    "# Retrain the model on the combined data\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n",
    "]\n",
    "\n",
    "# Fit the finetuned model (training)\n",
    "finetuned_history = finetuned_model.fit(\n",
    "    data, epochs=num_epochs, validation_data=validation_dataset, callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(finetuned_history.history[\"val_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWvBqrhQntYU"
   },
   "source": [
    "## Comparison against the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5qrftllntYU"
   },
   "outputs": [],
   "source": [
    "# The classification accuracies of the baseline and the pretraining + finetuning process:\n",
    "def plot_training_curves(finetuning_history, baseline_history):\n",
    "    for metric_key, metric_name in zip([\"acc\", \"loss\"], [\"accuracy\", \"loss\"]):\n",
    "        plt.figure(figsize=(8, 5), dpi=100)\n",
    "        plt.plot(\n",
    "            baseline_history.history[f\"val_{metric_key}\"],\n",
    "            label=\"supervised baseline\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            finetuning_history.history[f\"val_{metric_key}\"],\n",
    "            label=\"supervised finetuning\",\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.title(f\"Classification {metric_name} durforing training\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(f\"validation {metric_name}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_training_curves(finetuned_history, baseline_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base model\n",
    "results_baseline = baseline_model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Baseline Test Accuracy: {np.round(results_baseline[1] * 100,2)}%\")\n",
    "\n",
    "# Evaluate the finetuned model\n",
    "results_finetuned = finetuned_model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Finetuned Test Accuracy: {np.round(results_finetuned[1] * 100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline = []  # store predicted labels\n",
    "y_pred_finetuned = []  # store predicted labels\n",
    "y_true = []  # store true labels\n",
    "\n",
    "max_count = int(len(test_dataset))  # reduce dataset (change below)\n",
    "\n",
    "f = IntProgress(min=0, max=max_count)  # instantiate the bar\n",
    "display(f)  # display the bar\n",
    "\n",
    "# iterate over the dataset\n",
    "# for image_batch, label_batch in test_dataset.take(max_count):   # use dataset.unbatch() with repeat\n",
    "for image_batch, label_batch in test_dataset:  # use dataset.unbatch() with repeat\n",
    "    # append true labels\n",
    "    y_true.append(label_batch)\n",
    "    # compute predictions\n",
    "    preds = baseline_model.predict(image_batch, verbose=0)\n",
    "    # append predicted labels\n",
    "    y_pred_baseline.append(np.argmax(preds, axis=-1))\n",
    "\n",
    "    # compute predictions\n",
    "    preds = finetuned_model.predict(image_batch, verbose=0)\n",
    "    # append predicted labels\n",
    "    y_pred_finetuned.append(np.argmax(preds, axis=-1))\n",
    "\n",
    "    f.value += 1\n",
    "\n",
    "# convert the true and predicted labels into tensors\n",
    "correct_labels = tf.concat([item for item in y_true], axis=0)\n",
    "predicted_labels_baseline = tf.concat([item for item in y_pred_baseline], axis=0)\n",
    "predicted_labels_finetuned = tf.concat([item for item in y_pred_finetuned], axis=0)\n",
    "\n",
    "correct_labels = correct_labels.numpy().argmax(axis=1)\n",
    "\n",
    "# Generate reports\n",
    "matrix_baseline = confusion_matrix(correct_labels, predicted_labels_baseline)\n",
    "matrix_finetuned = confusion_matrix(correct_labels, predicted_labels_finetuned)\n",
    "report_baseline = classification_report(\n",
    "    correct_labels,\n",
    "    predicted_labels_baseline,\n",
    "    target_names=labels,\n",
    "    zero_division=0,\n",
    ")\n",
    "report_finetuned = classification_report(\n",
    "    correct_labels,\n",
    "    predicted_labels_finetuned,\n",
    "    target_names=labels,\n",
    "    zero_division=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example predictions from base- and finetuned-model\n",
    "num_images = 10\n",
    "\n",
    "images = next(iter(test_dataset))[0][:num_images]\n",
    "\n",
    "# Apply augmentations\n",
    "augmented_images = zip(\n",
    "    keras.Sequential([layers.Rescaling(1 / 255)])(images),\n",
    ")\n",
    "row_titles = [\n",
    "    \"Images:\",\n",
    "]\n",
    "plt.figure(figsize=(num_images * 1.3, 1.3), dpi=100)\n",
    "for column, image_row in enumerate(augmented_images):\n",
    "    for row, image in enumerate(image_row):\n",
    "        plt.subplot(1, num_images, row * num_images + column + 1)\n",
    "        plt.imshow(image)\n",
    "        if column == 0:\n",
    "            plt.title(row_titles[row], loc=\"left\")\n",
    "        plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"Baseline: {}\".format(\n",
    "        np.array(labels)[np.argmax(baseline_model.predict(images, verbose=0), axis=-1)]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Finetuned: {}\".format(\n",
    "        np.array(labels)[np.argmax(finetuned_model.predict(images, verbose=0), axis=-1)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(matrix_baseline, annot=True, cmap=\"viridis\", fmt='g')\n",
    "plt.xticks(ticks=np.arange(num_classes) + 0.5, labels=labels, rotation=90)\n",
    "plt.yticks(ticks=np.arange(num_classes) + 0.5, labels=labels, rotation=0)\n",
    "plt.title(\"Confusion Matrix (Baseline)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(matrix_finetuned, annot=True, cmap=\"viridis\", fmt='g')\n",
    "plt.xticks(ticks=np.arange(num_classes) + 0.5, labels=labels, rotation=90)\n",
    "plt.yticks(ticks=np.arange(num_classes) + 0.5, labels=labels, rotation=0)\n",
    "plt.title(\"Confusion Matrix (Finetuned)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report (Baseline):\\n\", report_baseline)\n",
    "print(\"Classification Report (Finetuned):\\n\", report_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "semisupervised_simclr",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
